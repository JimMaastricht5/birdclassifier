{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a689d0-54b5-4d04-b916-29f36f52746c",
   "metadata": {},
   "source": [
    "### Train Model Experiments NB \n",
    "This notebook contains the code to split the images and load them into train and validate sets.  The notebook also has the definition for the way the data should be loaded for each of six computer vision model types.  The notebook uses the concept of experiments to organize the fit/validate activities and allow for hyperparameter searching and logging results.  \n",
    "\n",
    "Scroll down to the head \"Experiments\" and add your experiment as a new cell underneath the heading, modify the experiment meta data and run the cell.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae7bf88-623d-49ab-ae35-c883cbe00ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 19:50:37.972047: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-19 19:50:38.748544: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-03-19 19:50:38.748629: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-03-19 19:50:38.748637: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# import the libraries for training, testing, validation\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import imagenet_utils  # will decode predictions out of the model into a 4 dim array of N (image num), imageID, label, probability result[0] would be the set of results for image one\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img  # will load img and reshape, usage is load_img(image_name_loc, target_size=input_shape)\n",
    "from tensorflow.keras.utils import plot_model  # Note: usage syntax is plot_model(instantied_model, to_file='', show_shapes=True)\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_datasets as tfds  # For loading datasets from GCS\n",
    "\n",
    "# import all model architectures\n",
    "from tensorflow.keras.applications import MobileNetV2, MobileNetV3Large, MobileNetV3Small\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB7\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9397c79d-8cba-44fd-b8bc-a367d54bad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary for model config and experiment tracking.... \n",
    "# define global variables\n",
    "gcs_bucket = 'nabirds_filtered'  \n",
    "dataset_path = 'images'  # Relative path within the bucket\n",
    "default_batch_size = 32\n",
    "default_image_size = (224, 224)\n",
    "\n",
    "model_input_variables = {\n",
    "    \"MobileNetV2\": {\n",
    "        \"input_shape\": (224, 224, 3),\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    "     \"MobileNetV3Large\": { #MobileNetV3 Large will be used to compare accuracy loss with small, prob not a candidate for rasp pi running\n",
    "        \"input_shape\": (224, 224, 3),\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    "    \"MobileNetV3Small\":{ \n",
    "        \"input_shape\": (224, 224, 3),\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    "    \"EfficientNetB0\": {\n",
    "        \"input_shape\": (224, 224, 3),\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    "    \"InceptionV3\": {\n",
    "        \"input_shape\": (299, 299, 3),  # InceptionV3 typically uses 299x299\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    "    \"EfficientNetB7\":{\n",
    "        \"input_shape\": (600, 600, 3),  # big values!\n",
    "        \"batch_size\": 16, # this is a larger model and the norm seems to be smaller batch sizes for reduced memory use\n",
    "    }\n",
    "}\n",
    "\n",
    "models_list = list(model_input_variables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29c6a513-1aad-4763-a833-c0eae28c78bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_results_to_file(filename, experiment, start_time, end_time, model_name, epochs, \n",
    "                          training_accuracy, validate_accuracy, training_loss, validate_loss,\n",
    "                          num_stages, stage1, stage2):\n",
    "    start_time_str = start_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    end_time_str = end_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    batch_size = str(model_input_variables[model_name]['batch_size'])\n",
    "    input_shape = str(model_input_variables[model_name]['input_shape']).replace(',','x')\n",
    "    stage1 = f\"stage1 epochs:{stage1['epochs']} trainable: {stage1['base_trainable']} trainable layers: {stage1['trainable_layers']} learning rate: {stage1['learning_rate']}\"\n",
    "    stage2 = f\"stage2 epochs:{stage2['epochs']} trainable: {stage2['base_trainable']} trainable layers: {stage2['trainable_layers']} learning rate: {stage2['learning_rate']}\"\n",
    "    line = f'{experiment},{start_time_str},{end_time_str},{model_name},{batch_size},{epochs},{input_shape},{training_accuracy},' \\\n",
    "           f'{validate_accuracy},{training_loss},{validate_loss},{num_stages},{stage1},{stage2}\\n'\n",
    "    if not os.path.exists(filename):      # Check if the file exists, and add a header if it's new\n",
    "        header = 'experiment,start_time,end_time,model_name,batch_size,epochs,input_shape,training_accuracy,validate_accuracy,training_loss,validate_loss,num_stages,stage1,stage2\\n'\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(header + line)\n",
    "    else:\n",
    "        with open(filename, \"a\") as f: # append to existing file\n",
    "            f.write(line)\n",
    "    print(f'experiment tracking updated')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293b3f33-f2f5-4147-a979-ee49b8c50341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_images_gcs(bucket_name, dataset_path, model_image_size, model_batch_size):\n",
    "    dataset = None\n",
    "    gcs_dataset_path = f\"gs://{bucket_name}/{dataset_path}\"\n",
    "    try:\n",
    "        dataset = keras.utils.image_dataset_from_directory(gcs_dataset_path, image_size=model_image_size,\n",
    "            batch_size=model_batch_size, label_mode='categorical',)  # categorical is for softmax layer\n",
    "    except Exception as e:\n",
    "        print(f'error loading dataset from gcs: {e}')    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3637ff1b-af98-4d1b-ab03-5fcf0051e528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split into its own cell so we do not have to repeat this long operation, takes 10 minutes....\n",
    "# need to run this each time for different image and batch sizes\n",
    "def load_images(image_size=default_image_size, batch_size=default_batch_size):\n",
    "    start_time = time.time()\n",
    "    start_time_datetime = datetime.datetime.fromtimestamp(start_time)\n",
    "    print(f'Start loading images time: {start_time_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "    print(f'Loading with image size of {image_size} and batch size of {batch_size}')\n",
    "\n",
    "    train_dataset = load_images_gcs(gcs_bucket, os.path.join(dataset_path, 'train'), image_size, batch_size)\n",
    "    validate_dataset = load_images_gcs(gcs_bucket, os.path.join(dataset_path, 'test'), image_size, batch_size)\n",
    "\n",
    "    if train_dataset is None or validate_dataset is None:\n",
    "        print(f'dataset loading failed.')\n",
    "    return train_dataset, validate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89bccfe4-decc-4edf-9120-e82ccfe42360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_training_results(acc, validate_acc, loss, validate_loss):\n",
    "    epochs_plt = range(1, len(acc) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_plt, acc, 'b', label='Training accuracy')\n",
    "    plt.plot(epochs_plt, validate_acc, 'r', label='Validation accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_plt, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs_plt, validate_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc17b88-aae1-4546-8266-c7618ad6ebe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lock and unlock layers, make this bullet proof so I don't have to think about the experiments too much\n",
    "def lock_unlock_layers(model, train_last_x_layers):\n",
    "    train_last_x_layers = 0 if train_last_x_layers == None else train_last_x_layers\n",
    "    fine_tune_at = len(model.layers) - abs(train_last_x_layers)                        \n",
    "    fine_tune_at = 0 if fine_tune_at < 0 else fine_tune_at\n",
    "    for layer in model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[fine_tune_at:]:\n",
    "        layer.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cfa5d68-42f8-4a13-947d-160b04046aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_model_experiment(experiment_name, model_name, stages, stage1, stage2, train_dataset, validate_dataset, num_classes):\n",
    "    stage1_learning = stage1[\"learning_rate\"]\n",
    "    stage1_epochs = stage1[\"epochs\"]\n",
    "    stage1_base_trainable = stage1['base_trainable']\n",
    "    stage1_trainable_layers = stage1['trainable_layers']\n",
    "    stage2_learning = stage2[\"learning_rate\"]\n",
    "    stage2_epochs = stage2[\"epochs\"]\n",
    "    stage2_base_trainable = stage2['base_trainable']\n",
    "    stage2_trainable_layers = stage2['trainable_layers']\n",
    "    \n",
    "    start_time = time.time()\n",
    "    start_time_datetime = datetime.datetime.fromtimestamp(start_time)\n",
    "    print(f'Start time training and validation: {start_time_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "    print(f'building model: {model_name} with {stages} stage(s)')\n",
    "    print(f'stage1... ')\n",
    "    print(f'epochs: {stage1_epochs}')\n",
    "    print(f'learning rate of {stage1_learning}')\n",
    "    print(f'trainable: {stage1_base_trainable}')\n",
    "    print(f'trainable_layers: {stage1_trainable_layers}')\n",
    "    \n",
    "     # load each mode type and modify head for softmax replacement\n",
    "    if model_name == 'MobileNetV2':\n",
    "        base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3)) \n",
    "\n",
    "    #####  setup stage 1 of X #####\n",
    "    base_model.trainable = stage1_base_trainable  # true unlocks the entire model\n",
    "    if stage1_base_trainable is False and stage1_trainable_layers != None:  # false a number unlocks the last x layers\n",
    "        base_model = lock_unlock_layers(model=base_model, train_last_x_layers=stage1_trainable_layers)\n",
    "    x = base_model.output  # start adding custom layers\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)  \n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # create the model with rescaling layer, this will automatically normalize the images within the model, bulletproofs feeder code\n",
    "    # normalize pixel values to [0, 1], not sure if this was done in the old mobilenetv2 model\n",
    "    inputs = base_model.input\n",
    "    rescaled_inputs = tf.keras.layers.Rescaling(1./255)(inputs)  \n",
    "    x = base_model(rescaled_inputs) # pass rescaled input through the base model\n",
    "    model = Model(inputs=inputs, outputs=predictions) # use original inputs, this is weird, but how it was done in tutorial...\n",
    "\n",
    "    # compile and train model, epoch zero is starting place for first stage, categorical crossentropy is for multi-class classification\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=stage1_learning), loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    history_phase1 = model.fit(train_dataset, epochs=stage1_epochs, initial_epoch=0, validation_data=validate_dataset)  \n",
    "\n",
    "    ##### setup stage 2 if requested\n",
    "    combined_history = {}\n",
    "    if stages == 2:\n",
    "        model = lock_unlock_layers(model=model, train_last_x_layers=stage2_trainable_layers)\n",
    "        # Recompile the model and continue from phase1, last epoch + 1 to move to next available \n",
    "        model.compile(optimizer=Adam(learning_rate=stage2_learning), loss='categorical_crossentropy', \n",
    "                      metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "        history_phase2 = model.fit(train_dataset, epochs=stage2_epochs, initial_epoch=history_phase1.epoch[-1] + 1, \n",
    "                                   validation_data=validate_dataset)   \n",
    "\n",
    "        for key in history_phase1.history:  # combine history\n",
    "            combined_history[key] = history_phase1.history[key] + history_phase2.history[key]\n",
    "    else:\n",
    "        combined_history = history_phase1  # if one state take stage 1 hist    \n",
    "                \n",
    "    #### process results and save model\n",
    "    # record training time\n",
    "    end_time = time.time()\n",
    "    end_time_datetime = datetime.datetime.fromtimestamp(end_time)\n",
    "    print(f'End time training and validation: {end_time_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "\n",
    "    total_epochs = stage1_epochs + stage2_epochs\n",
    "    model_filename = experiment_name + model_name\n",
    "    model.save(f'{model_filename}.h5')  # save the model\n",
    "\n",
    "    # write_results_to_file\n",
    "    acc = combined_history.history['accuracy']\n",
    "    validate_acc = combined_history.history['val_accuracy']\n",
    "    loss = combined_history.history['loss']\n",
    "    validate_loss = combined_history.history['val_loss']\n",
    "\n",
    "    write_results_to_file('experiment_log.csv', experiment_name, start_time_datetime, end_time_datetime, model_name, total_epochs, \n",
    "                          acc[-1], validate_acc[-1], loss[-1], validate_loss[-1], stages, stage1, stage2)\n",
    "    plot_training_results(acc, validate_acc, loss, validate_loss) \n",
    "    return acc, validate_acc, loss, validate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc7d276b-dcc1-4589-b863-3bb9e84946f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the experiments dictionary drives the model types and training parameters \n",
    "# loop over each model type, stages, and write out results of experiment to a file, save the model\n",
    "def run_experiments(experiments, train_dataset, validate_dataset, num_classes):\n",
    "    for exper in list(experiments.keys()):\n",
    "        stages = experiments[exper]['number_of_stages']\n",
    "        stage1 = experiments[exper]['stage1']\n",
    "        stage2 = experiments[exper]['stage2']\n",
    "        \n",
    "        for model_num in experiments[exper]['model_types']:\n",
    "            model_name = models_list[model_num]\n",
    "            acc, validate_acc, loss, validate_loss = run_model_experiment(exper, model_name, stages, stage1, stage2, train_dataset, validate_dataset, num_classes)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1025d43-4a0f-4b7d-acd8-6a700105079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['MobileNetV2', 'MobileNetV3Large', 'MobileNetV3Small', 'EfficientNetB0', 'InceptionV3', 'EfficientNetB7']\n",
      "EfficientNetB0: {'input_shape': (224, 224, 3), 'batch_size': 32}\n",
      "MobileNetV2: {'input_shape': (224, 224, 3), 'batch_size': 32}\n",
      "MobileNetV3Large: {'input_shape': (224, 224, 3), 'batch_size': 32}\n",
      "MobileNetV3Small: {'input_shape': (224, 224, 3), 'batch_size': 32}\n",
      "InceptionV3: {'input_shape': (299, 299, 3), 'batch_size': 32}\n",
      "EfficientNetB7: {'input_shape': (600, 600, 3), 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "### Available Models\n",
    "print(f'Available models: {models_list}')\n",
    "print(f'EfficientNetB0: {model_input_variables[\"EfficientNetB0\"]}')\n",
    "print(f'MobileNetV2: {model_input_variables[\"MobileNetV2\"]}')\n",
    "print(f'MobileNetV3Large: {model_input_variables[\"MobileNetV3Large\"]}')\n",
    "print(f'MobileNetV3Small: {model_input_variables[\"MobileNetV3Small\"]}')\n",
    "print(f'InceptionV3: {model_input_variables[\"InceptionV3\"]}')\n",
    "print(f'EfficientNetB7: {model_input_variables[\"EfficientNetB7\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c96d76ca-7817-4ca6-9186-e869b6e9d293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading images time: 2025-03-19 19:50:39\n",
      "Loading with image size of (224, 224) and batch size of 32\n",
      "Found 2455 files belonging to 27 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 19:55:10.065893: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-03-19 19:55:10.065925: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-03-19 19:55:10.065944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bird-feeder-profiling-2024-2025): /proc/driver/nvidia/version does not exist\n",
      "2025-03-19 19:55:10.066211: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 621 files belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "# load the default data 224x224x3 ,batch size 32 data set.  do not loop over this code unless its an exception. \n",
    "# takes about 10 minutes, note this will output warning/errors since this NB is not using a GPU\n",
    "# request a different load for a different model size, copy and paste code and change parameters\n",
    "default_train_dataset, default_validate_dataset = load_images()\n",
    "num_classes = len(default_train_dataset.class_names) # get class count, same no mater how the data is loaded given the same data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90a34f-90d0-40c1-9a77-8a32b37ddbeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiments\n",
    "Keep the experiments here modify the commented out code below and make that a new call.  The output include accuracy, precision, and recall for training and validate.  quick primer:  \n",
    "\n",
    "Accuracy: overall proportion of correct predictions (both true positives and true negatives) out of all predictions made.\n",
    "Formula: (True Positives + True Negatives) / (Total Predictions)\n",
    "\n",
    "Precision: measures the proportion of true positive predictions out of all positive predictions made by the model.\n",
    "Formula: True Positives / (True Positives + False Positives)\n",
    "\n",
    "Recall (also known as Sensitivity): measures the proportion of actual positive instances that the model correctly identified.\n",
    "Formula: True Positives / (True Positives + False Negatives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72bdf382-6702-4724-953b-ce1271716e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Sample Experiment, MobileNetV2 20 Epochs\n",
    "# models_list = list(model_input_variables.keys())\n",
    "# base_experiments_dict = {\n",
    "#     'sample MobileNetV2 20 Epochs': { \n",
    "#         # 'model_types': [0, 1, 2, 3], # leave out the two models with weird image sizes for now\n",
    "#         'model_types': [0], # mobilenetv2 only\n",
    "#         'number_of_stages': 1, \n",
    "#         'stage1': {'epochs': 20, 'base_trainable': True, 'trainable_layers': None, 'learning_rate': 0.0001},\n",
    "#         'stage2': {'epochs': 0, 'base_trainable': None, 'trainable_layers': None, 'learning_rate': 0}\n",
    "#         # 'stage2': {'epochs': 2, 'trainable': True, 'trainable_layers': -2, 'learning_rate': 0.00001}\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# run_experiments(base_experiments_dict, default_train_dataset, default_validate_dataset, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce338af-efa3-4b0d-92b0-ac425c11d6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb609827-b297-450f-bc1e-07ebaaf90a83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time training and validation: 2025-03-19 21:19:53\n",
      "building model: MobileNetV2 with 1 stage(s)\n",
      "stage1... \n",
      "epochs: 50\n",
      "learning rate of 0.0001\n",
      "trainable: True\n",
      "trainable_layers: None\n",
      "Epoch 1/50\n",
      "77/77 [==============================] - 112s 1s/step - loss: 1.9915 - accuracy: 0.4900 - precision_3: 0.9361 - recall_3: 0.1910 - val_loss: 2.9918 - val_accuracy: 0.1707 - val_precision_3: 0.3529 - val_recall_3: 0.0676\n",
      "Epoch 2/50\n",
      "13/77 [====>.........................] - ETA: 1:13 - loss: 0.6000 - accuracy: 0.8654 - precision_3: 0.9672 - recall_3: 0.7091"
     ]
    }
   ],
   "source": [
    "#### Base Experiment, MobileNetV2 50 Epochs, transfer learning, all layers traininable, learing rate 0.0001\n",
    "# converges quickly on training data (epoch 9 100%), but not validation data, indicates overfitting on the smaller training size\n",
    "models_list = list(model_input_variables.keys())\n",
    "base_experiments_dict = {\n",
    "    'base MobileNetV2 50 Epochs': { \n",
    "        # 'model_types': [0, 1, 2, 3], # leave out the two models with weird image sizes for now\n",
    "        'model_types': [0], # mobilenetv2 only\n",
    "        'number_of_stages': 1, \n",
    "        'stage1': {'epochs': 50, 'base_trainable': True, 'trainable_layers': None, 'learning_rate': 0.0001},\n",
    "        'stage2': {'epochs': 0, 'base_trainable': None, 'trainable_layers': None, 'learning_rate': 0}\n",
    "        # 'stage2': {'epochs': 2, 'trainable': True, 'trainable_layers': -2, 'learning_rate': 0.00001}\n",
    "    }\n",
    "}\n",
    "\n",
    "run_experiments(base_experiments_dict, default_train_dataset, default_validate_dataset, num_classes)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
